\documentclass[11pt, openany]{book}              % Book class in 11 points
\parindent0pt  \parskip10pt             % make block paragraphs
\raggedright                            % do not right justify


\title{\bf Machine Learning Handbook}    % Supply information
\author{Xinhe Liu}              %   for the title page.
\date{2018-2-28}                           %   Use current date. 

% Note that book class by default is formatted to be printed back-to-back.
\begin{document}                        % End of preamble, start of text.
% \frontmatter                            % only in book class (roman page #s)
\maketitle                              % Print title page.
\tableofcontents                        % Print table of contents
\mainmatter                             % only in book class (arabic page #s)

\part{High-level Views}
\chapter{Math Review}

\section{Linear Algebra}

Concepts:

\begin{itemize}
    \item scalar, vector, matrix, tensor(n-rank tensor, matrix is a rank 2 tensor) 
    \item Gaussian Elimination, rank
    \item p-norm $$|X|_p = (\sum_i |x_i|^p)^{\frac{1}{p}}$$
    \item inner product$<x_i,y_i>$, outer product
    \item orthogonal，dimension, basis, orthogonal basis
    \item linear transformation $Ax = y$
    \item eigenvalue, eigenvector $Ax =\lambda x$ (transformation and speed)
    \item vector space, linear space(with summation, scalar production), inner product space( inner product space) 
\end{itemize}


\section{Probability}

Concepts:

\begin{itemize}
    \item Classic Probability Model: Frequentist 
    \item Bayesian Probability Theory 
    \item Random variable, continuous RV, discrete RV, probability mass function, probability density function, cumulative density function
    \item Bernoulli distribution, Binomial distribution(n,p)$$P(X=k) = {N\choose k} p^k (1-p)^{(n-k)}$$, Poisson distribution $$P(X=k) = \lambda^k \frac{e^{-\lambda}}{k!}$$
    \item uniform distribution, exponential distribution $$e^{-\frac{x}{\theta}}\theta, P(x>s+t|X>s) = P(x>t)$$, normal distribution, t-distribution
    \item expectation, moments, variance, covariance, correlation coefficient
\end{itemize}


Theorems:

\begin{itemize}
    \item Law of Total Probability
    \item Bayesian Theorem $$P(H|D) = \frac{P(D|H) P(H)}{P(D)}$$ P(H)-prior probability, P(D|H)-likelihood, P(H|D)-posterior probability,
\end{itemize}

\subsection{Important Distributions, Moment Generating Functions}

\begin{enumerate}
	\item Normal Distribution, See next chapter
	\item Exponential Distribution $f_x(x,\theta) = \theta e^{-\theta x} $
	
\end{enumerate}


\section{Statistics}

\subsection{Concepts}

\begin{itemize}
    \item parameter(constant for probability model), statistic (model of sample data), data, sample, population
    \item point estimation, interval estimation, Confidence Interval( $P(L \leq \theta \leq U )$, notice: $\theta$ is not random, L, U is random! ( We repeat constructing confidence interval a n times, $\alpha$ percent of the times, it will contain $theta$.
    \item Estimator and Estimation
       \begin{itemize}
    	\item Method of Moments: $E(X^k)$ based on LOLN.\\ 
    	If We have p parameters, we can use p moments to form a system of equations to solve $\theta_1,...\theta_p$
    	$$\sum_{i=1}^n X_i^j= E(X^j)$$, for j = 1,...,p
    	\item Maximum Likelihood Estimation. Multiply p.m.f/p.d.f since every sample is independent. Maximize the likelihood of finding samples. 
    	$$ l(\theta) =  \prod_{i=1}^n f_{X_i} f_{x_i} (x_i; \theta), L(\theta ) = log l(\theta)$$
    	$$ \hat{\theta}_{MLE}= argmax_{\theta} f_x(x;\theta ) = argmax_{\theta} L(\theta )$$
    	Analytical or Numerically solved. $$ \frac{\partial}{\partial \theta} [log L(\theta ) ] = 0, \frac{\partial}{\partial \theta^2} [log L(\theta ) ] < 0$$, for multiple parameters, we need the Hessian matrix to be negative definite $x^tHx<0, \forall x$ 
    \end{itemize}
    \item Estimation criteria
   		\begin{itemize}
   		 \item Unbiased $E(\hat{\theta}) = \theta$
   		 \item Minimum Variance (MVUE, minimum variance unbiased estimator) $Var(\hat{\theta}) < Var(\theta')$
   		 \item Efficient
   		 \item Coherent
   		\end{itemize}
   	\item Hypothesis test, type-I error(wrongly reject), type-II error(wrongly accept)
\end{itemize}

\subsection{Important Distributions}

\begin{enumerate}
	\item Normal Distribution, $X_1,...X_n \sim N(\mu,\sigma^2)$ then
	\begin{enumerate}
		\item  $\bar{X}$ and $s^2$ are independent
		\item  $\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)$
		\item  $\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2$
		\item  $\frac{\bar{X}-\mu }{s/\sqrt{n}} = \frac{\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}}{\frac{(n-1)s^2}{\sigma^2} \frac{1}{\sqrt{n-1}}} \sim t_{n-1}$
	\end{enumerate}
	\item Multi-variate normal distribution
	$$ f_x(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$
	\begin{enumerate}
		\item $X_1,...X_n$ normal $\Leftarrow (X_1,...X_n)$ is multivariate normal. (Not equivalent)
		\item $E(X)=\mu, Var(X) = \Sigma$
		\item Linear transformations $AX+b \sim N(A\mu+b, A\Sigma A^T)$ remain multivariate normal
		\item Marginals are multivariate normal, each sub-vector is multivariate normal, the parameters are just sub-matrices.
		\item All conditionals are multivariate normal 
	\end{enumerate}
	\item t-distribution: like normal distribution, but heavier tails
	\begin{enumerate}
		\item $Z \sim N(0,1), Y \sim \chi^2_{\nu}$, Z, Y independent, $$ X= Z/\sqrt{Y/\nu} \sim t_{\nu} $$
		\item pdf has polynomial tails (decays much slower than exponential ones)
		\item $\nu=1$, it is the \textbf{Cauchy Distribution}, with very heavy tails (no expectation)
		\item The MCF not exist. $E(|X|^k) < \infty$ for $k<\nu$, $E(|X|^k) = \infty$ for $k>\nu$
		\item $X\sim t_\nu, E(X)=0, Var(X)=\frac{\nu}{\nu-2}$
		$$ f_X(x)=\frac{1}{\pi(1+x^2)}$$
	\end{enumerate}
	\item $\chi^2$ distribution
	$$ f_x(x) = \frac{1}{(2^{k/2}\Gamma(k/2)}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}, x\in [0,\infty) \sim Gamma(\frac{k}{2},\frac{1}{2})$$
	\begin{enumerate}
		\item $E(X)=k, Var(X)=2k, M_X(t)= (\frac{1}{1-2^t})^{k/2}$
		\item $X \sim N(0,1) \Rightarrow X^2 \sim \chi^2$, $X_1,...X_n \sim N(0,1) i.i.d \Rightarrow \sum X_i^2 \sim \chi^2$,
		$$ f_X(x)=\frac{1}{\pi(1+x^2)}$$
	\end{enumerate}
	
\end{enumerate}


\subsection{Theorems}

\begin{itemize}
    \item Law of Large Number
    \item Central Limit Theorem
        \item Bias/Variance decomposition (error = bias + variance + noise)
        $$MSE(\mu(X) ) = E[(Y-\hat{\mu}(X))^2]  = = E[(Y-f(x) + f(x) -\hat{\mu}(X))^2]$$
    $$= E[(Y-f(x)]^2 + 2E[(Y-f(x))(f(x) - \hat{\mu}(X))] + E[(f(x)-\hat{\mu}(X))^2]$$
     $$= E[(Y-f(x)]^2 + 2E[(Y-f(x))(f(x) - \hat{\mu}(X))] + (f(x)-\hat{\mu}(X))^2$$
    $$ =\sigma_x^2 + Bias(\hat{\mu}(X))^2 + Var(\hat{\mu}(X))$$
 
\end{itemize}
   
\subsection{Practice/Examples}

\begin{itemize}
    \item sample mean($\bar{X}$).is unbiased. Sample variance ($\frac{1}{n-1}\sum_{i=1}^n x_i^n$) is unbiased. But sample std is not unbiased. $SE(\bar{X})=\frac{\sigma^2}{n}$
    \item $\hat{Cov}(X.Y) = \frac{1}{n-1} \sum_{i=1}^n(X_i-\bar{X})(X_i-\bar{Y})$ is unbiased
    \item Distributions with Expectation not exist? (Cauchy) 
    \item Common Confidence Intervals:\\ $\mu$: $P(-t_{\alpha/2, n-1} \leq \frac{\bar{X}-\mu}{s/\sqrt{n}} \leq t_{\alpha/2, n-1})= 1- \alpha$, \\$\sigma$: $P(a \leq \frac{(n-1)s^2}{\sigma^2} \leq b)= 1-\alpha$
    \item Solve MLE/MOM for beta, exponential ($n/\sum{X_i}$, normal 
 
\end{itemize}
 
 
\section{Optimization Theory}

\begin{itemize}
    \item Objective function/Evaluation function, constrained/unconstrained optimization，Feasible Set, Optimal Solution, Optimal Value, Binding Constraints, Shadow Price, Infeasible Price, Infeasibility, Unboundedness    
    \item Linear Programming
    \item Lagrange Multiplier $$L(x,y,\lambda) = f(x,y) + \lambda \varphi(x,y) $$
    \item Convex Set, Convex Function $f:S\to R$ is convex if and only if $\bigtriangledown^2 f(\mathbf{x})$ is positive semidefinite 
\end{itemize}

Optimization Methods:
\begin{itemize}
    \item Linear Search Method: Direction First, Step Size second	
    	\begin{itemize}  		
    		\item Gradient Descent: Batch Processing(Use all samples) vs Stochastic Gradient Descent(Use one sample)
    		\item Newton's Method: Use Curvature Information 
   		\end{itemize}
   	\item Trust Region: Step first, direction second. Find optimal direction of second-order approximation. If the descent size is too small, make step size smaller.
   	\item Heuristics Method
   		\begin{itemize}  		
	    	\item Genetic Algorithm
	 	   	\item Simulated Annealing 
	    	\item Partical Swarming/Ant Colony Algorithm
	    \end{itemize}
\end{itemize}

Theorems:

\begin{itemize}
	\item
\end{itemize}

\section{Information Theory}

Concepts:

\begin{itemize}
    \item Information $$h(A) = -log_2 p(A)$$ (bit)
    \item (Information Source) Entropy $$ H(X) = -\sum_{i=1}^n p(a_i)log_2p(a_i) \leq log_2 n$$ Maximize under equal probability
    \item Conditional Entropy $$H(Y|X)  = -\sum_{i=1}^n p(x_i)H(Y|X=x_i)= -\sum_{i=1}^n p(x_i)\sum_{j=1}^n p(y_j|x_i) log_2p(y_j|x_i) $$ $$ = \sum_{i=1}^n \sum_{j=1}^n p(x_i,y_j) log_2p(y_j|x_i) $$
    \item Mutual Information/Information Gain $$I(X;Y) = H(Y) - H(Y|X)$$
    \item Kullback-Leibler Divergence (K-L) Divergence $$D_{KL}(P||Q) = \sum_{i=1}^n p(x_i) log_2\frac{p(x_i)}{q(x_i)} \neq D_{KL}(Q||P)$$
    	Measures the Distance of two distributions. The optimal encoding of information has the same bits as the entropy. Measures the extra bits if the real distribution is q rather than p. (Using P to approximate Q)
\end{itemize}


Theorems:

\begin{itemize}
    \item The Maximum Entropy Principle. Without extra assumption, max entropy/equal probability has the minimum prediction risk. 
\end{itemize}

\section{Formal Logic}

Concepts
\begin{itemize}
    \item Generative Expert System: Rule+Facts+Deduction Engine 
    \item Godel's incompleteness theorems
\end{itemize}

\chapter{Computational Learning Theory}

\part{Supervised Learning Models}

\chapter{Regression}

\section{Linear Regressions}


\subsection{Assumptions}
Classic Assumptions for Statistics:
\begin{itemize}
    \item 
\end{itemize}

\subsection{Inteprataion}

$$ f(x) = \mathbf{w}^T \mathbf{x} = \sum_{i=1}^n w_i x_i $$
$$ \mathbf{w*} = (\mathbf{X^T X})^{-1} \mathbf{X^T y} $$

RSS Approach:


MLE Approach

Assuming noise is normal, maximize 

$$p(\mathbf{x_1, x_2 ... x_n}| \mathbf{ w} ) = \prod_k \frac{1}{\sqrt{2\pi} \sigma} exp[ -\frac{1}{2\sigma^2} (y_k - \mathbf{w_t x_k} )^2 ]$$ 

\subsection{Lasso-Least Absolute Shrinkage and Selection Operator}

$$ min ||y_k - \mathbf{w^T x}_k ||^2 + \lambda ||\mathbf{w}||_1 $$

\chapter{Logistic Regression and General Linear Model}

\chapter{Naive Bayesian}

\chapter{Tree Models and Ensemble Learning}

\part{Unsupervised Learning Models}

\chapter{Clustering}

\chapter{Dimension Reduction}



\part{Deep Learning and Enhanced Learning Theory}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

 
\end{document}                          % The required last line 