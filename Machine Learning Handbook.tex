\documentclass[11pt, openany]{book}              % Book class in 11 points
\parindent0pt  \parskip10pt             % make block paragraphs
\raggedright                            % do not right justify
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}  % in your preamble
\DeclareMathOperator*{\argmin}{arg\,min}  % in your preamble 

\title{\bf Machine Learning Handbook}    % Supply information
\author{Xinhe Liu}              %   for the title page.
\date{2018-2-28}                           %   Use current date. 

% Note that book class by default is formatted to be printed back-to-back.
\begin{document}                        % End of preamble, start of text.
% \frontmatter                            % only in book class (roman page #s)
\maketitle                              % Print title page.
\tableofcontents                        % Print table of contents
\mainmatter                             % only in book class (arabic page #s)

\part{High-level Views}
\chapter{Math Review}

\section{Linear Algebra}

Concepts:

\begin{itemize}
    \item scalar, vector, matrix, tensor(n-rank tensor, matrix is a rank 2 tensor) 
    \item Gaussian Elimination, rank
    \item p-norm $$|X|_p = (\sum_i |x_i|^p)^{\frac{1}{p}}$$
    \item inner product$<x_i,y_i>$, outer product
    \item orthogonal，dimension, basis, orthogonal basis
    \item linear transformation $Ax = y$
    \item eigenvalue, eigenvector $Ax =\lambda x$ (transformation and speed)
    \item vector space, linear space(with summation, scalar production), inner product space( inner product space) 
\end{itemize}


\section{Probability}

Concepts:

\begin{itemize}
    \item Classic Probability Model: Frequentist 
    \item Bayesian Probability Theory 
    \item Random variable, continuous RV, discrete RV, probability mass function, probability density function, cumulative density function
    \item Bernoulli distribution, Binomial distribution(n,p)$$P(X=k) = {N\choose k} p^k (1-p)^{(n-k)}$$, Poisson distribution $$P(X=k) = \lambda^k \frac{e^{-\lambda}}{k!}$$
    \item uniform distribution, exponential distribution $$e^{-\frac{x}{\theta}}\theta, P(x>s+t|X>s) = P(x>t)$$, normal distribution, t-distribution
    \item expectation, moments, variance, covariance, correlation coefficient
\end{itemize}


Theorems:

\begin{itemize}
    \item Law of Total Probability
    \item Bayes' Rule $$P(H|D) = \frac{P(D|H) P(H)}{P(D)}$$ P(H)-prior probability, P(D|H)-likelihood, P(H|D)-posterior probability,
\end{itemize}

\subsection{Important Distributions, Moment Generating Functions}

\begin{enumerate}
	\item Normal Distribution, See next chapter
	\item Bernoulli Distribution 
	\item Exponential Distribution $f_x(x,\theta) = \theta e^{-\theta x} $
	\item Poisson Distribution
	
\end{enumerate} 
\section{Information Theory}

Concepts:

\begin{itemize}
    \item Information $$h(A) = -log_2 p(A)$$ (bit)
    \item (Information Source) Entropy $$ H(X) = -\sum_{i=1}^n p(a_i)log_2p(a_i) \leq log_2 n$$ Maximize under equal probability
    \item Conditional Entropy $$H(Y|X)  = -\sum_{i=1}^n p(x_i)H(Y|X=x_i)= -\sum_{i=1}^n p(x_i)\sum_{j=1}^n p(y_j|x_i) log_2p(y_j|x_i) $$ $$ = \sum_{i=1}^n \sum_{j=1}^n p(x_i,y_j) log_2p(y_j|x_i) $$
    \item Mutual Information/Information Gain $$I(X;Y) = H(Y) - H(Y|X)$$
    \item Kullback-Leibler Divergence (K-L) Divergence 
      $$D_{KL}(P||Q) = \sum_{i=1}^n p(x_i) log_2\frac{p(x_i)}{q(x_i)} \neq D_{KL}(Q||P)$$
      $$D_{KL}(f,\hat{f})) = \int_{-\infty}^{\infty} log(\frac{f_X(x)}{\hat{f(x)}})f_X(x)dx $$
    Measures the Distance of two distributions. The optimal encoding of information has the same bits as the entropy. Measures the extra bits if the real distribution is q rather than p. (Using P to approximate Q) K-L divergence plays an important role in both information theory and MLE theory. MLE $\hat{\theta}$ is actually finding the closest K-L Distance approximation of $f(x;\theta)$ to sample distribution.
\end{itemize}


Theorems:

\begin{itemize}
    \item The Maximum Entropy Principle. Without extra assumption, max entropy/equal probability has the minimum prediction risk. 
\end{itemize}
 
\section{Optimization Theory}

\begin{itemize}
    \item Objective function/Evaluation function, constrained/unconstrained optimization，Feasible Set, Optimal Solution, Optimal Value, Binding Constraints, Shadow Price, Infeasible Price, Infeasibility, Unboundedness    
    \item Linear Programming
    \item Lagrange Multiplier $$L(x,y,\lambda) = f(x,y) + \lambda \varphi(x,y) $$
    \item Convex Set, Convex Function $f:S\to R$ is convex if and only if $\bigtriangledown^2 f(\mathbf{x})$ is positive semidefinite 
\end{itemize}

Optimization Methods:
\begin{itemize}
    \item Linear Search Method: Direction First, Step Size second	
    	\begin{itemize}  		
    		\item Gradient Descent: Batch Processing(Use all samples) vs Stochastic Gradient Descent(Use one sample)
    		\item Newton's Method: Use Curvature Information 
   		\end{itemize}
   	\item Trust Region: Step first, direction second. Find optimal direction of second-order approximation. If the descent size is too small, make step size smaller.
   	\item Heuristics Method
   		\begin{itemize}  		
	    	\item Genetic Algorithm
	 	   	\item Simulated Annealing 
	    	\item Partical Swarming/Ant Colony Algorithm
	    \end{itemize}
\end{itemize}

Theorems:

\begin{itemize}
	\item
\end{itemize}


\section{Formal Logic}

Concepts
\begin{itemize}
    \item Generative Expert System: Rule+Facts+Deduction Engine 
    \item Godel's incompleteness theorems
\end{itemize}

\chapter{Statistics}

\section{Concepts}
\subsection{Basic}
\begin{itemize}
    \item parameter(constant for probability model), statistic (model of sample data), data, sample, population
    \item point estimation, interval estimation, Confidence Interval( $P(L \leq \theta \leq U )$, notice: $\theta$ is not random, L, U is random! ( We repeat constructing confidence interval a n times, $\alpha$ percent of the times, it will contain $theta$.
\end{itemize}

\subsection{Estimator and Estimation}

\begin{itemize}
    \item Method of Moments: $E(X^k)$ based on LOLN.\\ 
    	If We have p parameters, we can use p moments to form a system of equations to solve $\theta_1,...\theta_p$
    	$$\sum_{i=1}^n X_i^j= E(X^j)$$, for j = 1,...,p
    \item Maximum Likelihood Estimation. Multiply p.m.f/p.d.f since every sample is independent. Maximize the likelihood of finding samples. \\ If $X_1,...X_n \stackrel{i.i.d}{\sim} f_x(x, \theta)$, 
    	$$ l(\theta) =  \prod_{i=1}^n f_{X_i} f_{x_i} (x_i; \theta), L(\theta ) = log l(\theta)$$
    	$$ \hat{\theta}_{MLE}= argmax_{\theta} f_x(x;\theta ) = argmax_{\theta} L(\theta )$$
    	Analytical or Numerically solved. $$ \frac{\partial}{\partial \theta} [log L(\theta ) ] = 0, \frac{\partial^2}{\partial \theta^2} [log L(\theta ) ] < 0$$, for multiple parameters, we need the Hessian matrix to be negative definite $x^tHx<0, \forall x$ \\
    \item Properties of MLE
    	\begin{enumerate}
    		\item Invariance $\hat{\theta}$ is MLE of $\theta$, then $g(\hat{\theta})$ is MLE of $g(\theta)$
    		\item Consistency $$P(\hat{\theta} - \theta ) \rightarrow 0$$ as $n \rightarrow 0, \forall \epsilon > 0$ Under the conditions
    		\begin{enumerate}
    			\item $X_1,...X_n \stackrel{i.i.d}{\sim} f_x(x|\theta)$
    			\item parameters are identifiable, $\theta \neq \theta', f_x(x|\theta) \neq f_x(x|\theta')$
    			\item densities $f_x(x|\theta)$ has common support(set of x with positive density/probability), $f_x(x|\theta)$ is differentiable at $\theta$
    			\item parameter space $\Omega$ contains open set $\omega$ where true $\theta_0$ is an interior point
    		\end{enumerate}
    		\item Asymptotic Normality 
    		$$ \sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \rightarrow N(0, I^{-1}(\theta_0)) $$
    		$$ I(\theta_0) = E( -(\frac{\partial}{\partial \theta} [log f(x, \theta ) ])^2)=E(-\frac{\partial^2}{\partial \theta^2} [log f(x, \theta ) ] )$$ 
    		called the Fisher Information
    		$$ \hat{\theta}_{MLE} \approx N(\theta_0, \frac{1}{n I(\theta_0)})$$
    		$$ n I(\theta_0) = E( -\frac{\partial^2}{\partial \theta^2} log L(\theta) )$$
    		So the Variance of MLE( $1/ E( -\frac{\partial^2}{\partial \theta^2} log L(\theta) )$) is the reciprocal of amount of curvature at MLE. \\
    		Usually, We can just use the \textit{observed Fisher Information} (curvature near $\theta_{MLE}$) instead. ($I(\theta_{MLE})$) \\
    		$\frac{1}{ n I(\theta_0)}$ is called Cramer-Rao Lower Bound. \\ Under Multi-dimensional Case, 
    		
    		$$ I(\theta_0)_{ij} = =E(-\frac{\partial^2}{\partial \theta_i \partial \theta_j} [log f(x, \theta ) ] )$$ $Hessian \approx nI(\theta_0)$ $Hessian^{-1} \approx nI(\theta_0)$ when we use numerical approach. 

    		Under the above four conditions plus 
    		\begin{enumerate}
    			\item $ \forall x \in \chi$, $f_x(x|\theta)$ is three times differentiable with respect to $\theta$, and third derivative is continuous at $\theta$, and $\int f_x(x|\theta) dx$ can be differentiated three times under integral sign
    			\item $ \forall \theta \in \Omega, \exists c, M(x)$ (both depends on $\theta_0$) such that
    			$$ \frac{\partial^3}{\partial \theta^3} [log f(x, \theta ) ] \leq M(x), \forall x \in \chi, \theta_0-c<\theta < \theta_0+c, E_{\theta_0} [M(x)] < \infty$$
    		\end{enumerate}
    	\end{enumerate}
    \item $\Delta$ -Method: $g(\hat{theta}_{MLE})$ is approximately
    	  $$ N(g(\theta), (g'(\theta))^2 \frac{1}{nI(\theta)})$$ if asymptotic normality is satisfied. \\
    	  In Multivariate Case:
    	  $$ \hat{\theta} \sim N(\theta, \Sigma/n), \theta,\hat{\theta} \in R^p $$
    	  $$g: R^p \rightarrow R^m$$
    	  $$g(\hat{\theta}) \sim N(g(\theta), G\Sigma G^T/n)$$
		  $$ G = \begin{pmatrix} 
  				\frac{\partial{g_1(\theta)}}{\partial{\theta_1}}& \cdots & \frac{\partial{g_1(\theta)}}{\partial{\theta_p}}\\ 
  				\vdots & \ddots & \vdots \\
  				\frac{\partial{g_m(\theta)}}{\partial{\theta_1}}& \cdots & \frac{\partial{g_m(\theta)}}{\partial{\theta_p}} 
			\end{pmatrix} $$
    \item Estimation criteria
   		\begin{itemize}
   		 \item Unbiased $E(\hat{\theta}) = \theta$
   		 \item Minimum Variance (MVUE, minimum variance unbiased estimator) $Var(\hat{\theta}) < Var(\theta')$
   		 \item Efficient
   		 \item Coherent
   		 \end{itemize}
\end{itemize}

\subsection{Model Selection}

AIC - Akaike Information Criterion

By K-L Distance
$$D_{KL}(f,\hat{f})) = \int_{-\infty}^{\infty} log(\frac{f_X(x)}{\hat{f(x)}})f_X(x)dx $$
$$ = const + \frac{1}{2} \int (-2log\hat{f}(x))f(x)dx = const + AIC $$
$$ A(f,\hat{f}) = -2 logL(\theta) + 2 p (\frac{n}{n-p+1})$$

\subsection{Hypothesis Testing}

\begin{itemize}
    \item Hypothese, Test Statistic(T), Rejection Region
    \item p-value ( chance of rejecting, largest choice of $\alpha$ that we would fail to reject $H_0$)   
    \item type-I error(wrongly reject), type-II error(wrongly accept)
\end{itemize}

Hypothesis Testings (Based on the distribution of $\hat{\theta}$)

\begin{itemize}
    \item Wald Test $$T = \frac{\hat{\theta} - \theta_0}{Se(\hat{\theta})}$$
    $$ \hat{\theta}_{MLE} \approx N(\theta_0, \frac{1}{n I(\theta_0)})$$
   $$T = \frac{\hat{\theta} - \theta_0}{\sqrt{\frac{1}{nI(\theta_0)})}}$$
    \item Likelihood Ratio Test
    \item Score Test
\end{itemize}

*Computation-based hypothesis testing approach

\begin{itemize}
    \item Permutation tests: \\ Test $X_1,..X_n \sim F, Y_1,...Y_n \sim G, if F=G$. Use $T = Mean(X_i )- Mean(Y_i)$, each time scramble X and V labels and should not not change the distributions of vectors $X_1,...X_n, Y_1...,Y_n$
    \item Bootstrapping: \\ $X_1,...X_n \sim F$ with $T = T(X_1,...,X_n)$, to get the distribution of T, \textbf{sample with replacement.} The belief is $(\hat{\theta} - \theta)$ should behave the same as $(\theta* - \hat{theta})$. The first quantity can be treated like a pivot.  (use $(\theta*_1 - \hat{\theta}_1),...(\theta*_n - \hat{\theta}_n)$ to test. 
\end{itemize}

Multiple Testing 

\begin{itemize}
    \item Family-wise Error Rate(FWER) the probability of rejecting at least one of at least one null hypothesis \\ 
    Under independence, the probability of making mistake when all null are true: (P( any type I mistake) = 1-P(no type I mistake for all) = $1-(1-\alpha)^M=\beta$) \\
    \item Bonferroni correction, assuming independence $P(\bigcup_{i=1}^n {typeI_mistake}) \leq \sum_{i=1}^n P(typeI_mistake) \leq M\alpha$, control at $\alpha=\frac{\alpha}{M}$ \\
    $\alpha$ being to small will impact power of the individual tests!
    \item False Discovery Rate(FDR): bound the fraction of type-I errors. R be the total number of hypotheses rejected. V be the number of rejected hypotheses that were actually null. Let FDR = V/max(R,1), control $E(FDR) \leq \alpha$.
\end{itemize}

\section{Theorems}
\begin{itemize}
    \item Law of Large Number
    \item Central Limit Theorem
        \item Bias/Variance decomposition (error = bias + variance + noise)
        $$MSE(\mu(X) ) = E[(Y-\hat{\mu}(X))^2]  = = E[(Y-f(x) + f(x) -\hat{\mu}(X))^2]$$
    $$= E[(Y-f(x)]^2 + 2E[(Y-f(x))(f(x) - \hat{\mu}(X))] + E[(f(x)-\hat{\mu}(X))^2]$$
     $$= E[(Y-f(x)]^2 + 2E[(Y-f(x))(f(x) - \hat{\mu}(X))] + (f(x)-\hat{\mu}(X))^2$$
    $$ =\sigma_x^2 + Bias(\hat{\mu}(X))^2 + Var(\hat{\mu}(X))$$
\end{itemize}

\section{Important Distributions}

\begin{enumerate}
	\item Normal Distribution, $X_1,...X_n \sim N(\mu,\sigma^2)$ then
	\begin{enumerate}
		\item  $\bar{X}$ and $s^2$ are independent
		\item  $\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)$
		\item  $\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2$
		\item  $\frac{\bar{X}-\mu }{s/\sqrt{n}} = \frac{\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}}{\frac{(n-1)s^2}{\sigma^2} \frac{1}{\sqrt{n-1}}} \sim t_{n-1}$
	\end{enumerate}
	\item Multi-variate normal distribution
	$$ f_x(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$
	\begin{enumerate}
		\item $X_1,...X_n$ normal $\Leftarrow (X_1,...X_n)$ is multivariate normal. (Not equivalent)
		\item $E(X)=\mu, Var(X) = \Sigma$
		\item Linear transformations $AX+b \sim N(A\mu+b, A\Sigma A^T)$ remain multivariate normal
		\item Marginals are multivariate normal, each sub-vector is multivariate normal, the parameters are just sub-matrices.
		\item All conditionals are multivariate normal 
	\end{enumerate}
	\item t-distribution: like normal distribution, but heavier tails
	\begin{enumerate}
		\item $Z \sim N(0,1), Y \sim \chi^2_{\nu}$, Z, Y independent, $$ X= Z/\sqrt{Y/\nu} \sim t_{\nu} $$
		\item pdf has polynomial tails (decays much slower than exponential ones)
		\item $\nu=1$, it is the \textbf{Cauchy Distribution}, with very heavy tails (no expectation)
		\item The MCF not exist. $E(|X|^k) < \infty$ for $k<\nu$, $E(|X|^k) = \infty$ for $k>\nu$
		\item $X\sim t_\nu, E(X)=0, Var(X)=\frac{\nu}{\nu-2}$
		$$ f_X(x)=\frac{1}{\pi(1+x^2)}$$
	\end{enumerate}
	\item $\chi^2$ distribution
	$$ f_x(x) = \frac{1}{(2^{k/2}\Gamma(k/2)}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}, x\in [0,\infty) \sim Gamma(\frac{k}{2},\frac{1}{2})$$
	\begin{enumerate}
		\item $E(X)=k, Var(X)=2k, M_X(t)= (\frac{1}{1-2^t})^{k/2}$
		\item $X \sim N(0,1) \Rightarrow X^2 \sim \chi^2$, $X_1,...X_n \sim N(0,1) i.i.d \Rightarrow \sum X_i^2 \sim \chi^2$,
		$$ f_X(x)=\frac{1}{\pi(1+x^2)}$$
	\end{enumerate}	
\end{enumerate}

More Generalized Distributions

\begin{enumerate}
	\item Generalized Error Distribution (symmetric)
	\item Non-standard t-distribution (shift and scaling, heavy tailed, symmetric)
	\item Theodossious skewed t-distribution
	\item Theodossious skewed t-distribution plus shift
\end{enumerate}


\section{Practice/Examples}

\begin{enumerate}

    \item sample mean($\bar{X}$) is unbiased. Sample variance ($\frac{1}{n-1}\sum_{i=1}^n x_i^n$) is unbiased. But sample std is not unbiased. $SE(\bar{X})=\frac{\sigma^2}{n}$
    \item $\hat{Cov}(X.Y) = \frac{1}{n-1} \sum_{i=1}^n(X_i-\bar{X})(X_i-\bar{Y})$ is unbiased
    \item Distributions with Expectation not exist? (Cauchy) 
    \item Common Confidence Intervals:\\ $\mu$: $P(-t_{\alpha/2, n-1} \leq \frac{\bar{X}-\mu}{s/\sqrt{n}} \leq t_{\alpha/2, n-1})= 1- \alpha$, \\$\sigma$: $P(a \leq \frac{(n-1)s^2}{\sigma^2} \leq b)= 1-\alpha$
    \item Solve MLE/MOM for beta, exponential ($n/\sum{X_i}$, normal 
    \item * prove Asymptotic Normality of MLE( hint: using Taylor Expansion for $\theta, \hat{\theta}$ )
    \item * Use $t^{th}$ quantile to approximate c.d.f, what's the distribution? ($Y_n=\frac{1}{n} \sum I(X_i <x)$, a Bernoulli distribution with $p=F_x(x)$, $\sqrt{n}[Y_n(x) - F_x(x)] \sim N(0, F(x)(1-F(x))$.
 	\item $X_1, .... X_n \sim Binomial(n,p)$, What's the MLE for p and Fisher Information? ( $\hat{p}= \frac{x_i}{n}, I(p) = 1/p(1-p), var(p) = \frac{p(1-p)}{n}$)
 	\item $(x_i,y_i) \sim N(\mu_i, \sigma^2)$, find MLE for $\sigma$ ( $\frac{1}{4N} \sum(x_i-y_i)$ )
 	\item How can you get N(0,1) random variables from U[0,1]? ( Method1: Inverse Transformation, Method2; Use $Sum Z_i^2 \sim \chi_k^2, k = 2, F^{-1}(u) = -2log(1-u)$, $R^2 \sim \chi^2, Z_1 = Rcos\theta, Z_2 = Rsin\theta, \theta \in [0, 2\pi]$
 	\item (Permutation test) how can you test $X_1,...,X_n \sim F$, how can you test if F is symmetric? (Multiply -1 on all two form two sample groups)
 	\item Draw a bootstrap sample, what fraction of original data points appear in this sample on average?  
 	\\ Define I be the indicator is it is in the sample. $E(\frac{1}{n}\sum I_i= E(I_i) = P(\text{ith point shows up})= 1- (1-\frac{1}{n})^n$
\end{enumerate}

\chapter{Bayesian Statistical Theory}

Bayes' Rule 
$$ f_{Y|X}(y|x) = \frac{f_{(X,Y)}(x,y)}{f_X(x)} = \frac{f_{(X,Y)}(x,y)}{\int f(x|y)f(y)dy}$$
Byesian Inference:\\
All parameters are random variables,
$$\pi(\theta|x) =  \frac{f(x|\theta) \pi(\theta)}{\int f(x|\theta)\pi(\theta)d\theta}$$
$$\pi(\theta|x) \sim  f(x|\theta) \pi(\theta)$$
$\pi(\theta)$ is the prior distribution, $\pi(\theta|x)$ is the posterior distribution for $\theta$ given x. 

Bayes Estimator
$$\hat{\theta}_{Bayes} = E(\theta | X) = \int \theta \pi(\theta |X ) d\theta$$

Conjugate Distribution:
$f(x), \pi$ is called conjugate distributions if model $\pi(\theta|x), \pi(\theta)$ follows the same Distribution

eg. Bernoulli($\theta$) and Beta($\alpha,\beta$),  ( $\pi(\theta|x) \sim$ Beta($\alpha+\sum X_i, \beta + n - \sum X_i$) ($f(x|\theta) = \prod_{i=1}^n f_{X_i}(X_i|\theta)$)
$$\hat{\theta}_{Bayes} = E(\theta | X) = \frac{ \alpha + \sum X_i}{ \alpha + \beta + n} $$
$$= \frac{\sum X_i}{n}\frac{n}{\alpha + \beta + n} + \frac{\alpha}{\alpha + \beta}\frac{ \alpha + \beta}{\alpha + \beta + n}$$

The prior mean ( second term ) influences less as n grows.

Poisson($\theta$) and $Gamma(\alpha + \sum X_i, \beta + n )$ 

\section{ Bayesian Decision Theory}

In State $\omega$ we take action $a \in A$, incurr Loss $L(\omega,a)$, how to choose a?
Risk:
$$R(a|x) = \sum_{j=1}^k L(\omega_j, a) P(\omega_j |x )$$
Decision Rule $d \in A$
$$d^*(x) = \argmin_{a \in A} R(a|x)$$
\chapter{Computational Learning Theory}

\part{Supervised Learning Models}

\chapter{Regression Overview and Linear Regression}

\section{Overview}

All Basic Models begins with \textbf{Linear Regression} Because

\begin{itemize}
    \item Linear relationship is the simplest relationship other than constant relationship or "null" model (average)
    \item It's a global model
    \item Data Invariance: Simple linear model don't do any pre-processing or transformation on the covariants.
    \item Very Explainable, limited interpretation power.
\end{itemize}
 
 So, the alternation/improvements also focuses on these aspects
 
 \begin{itemize}
 	\item Nonlinear features-Introduction of basis function
 	\begin{itemize}
    	\item Polynomial Regression
    	\item Spline Models(eg. Cubic Spline, Smoothing Spline)
	\end{itemize}
 	\item Nonlinear parameters: Parameters Self-adjusting.(activation function is an example of basis function as well)
 	 \begin{itemize}
    	\item Neutral-Network
	\end{itemize}
 	\item global nonlinear: global nonlinear on both parameters and features achieved by linkage function, extends regression models to classification.
 	 \begin{itemize}
    	\item Generalized Linear Model
	\end{itemize}
    \item Change the global model to a local model
   	\begin{itemize}
    	\item Local Regression ( Regression + KNN) 
    	\item Nonparametric Regression
    	\item Kernel Function 
    	\item Distance Based Learning
	\end{itemize}	
	\item Data Preprocessing (Transformation) and Dimension Reduction
   	\begin{itemize}
    	\item PCA
    	\item LDA
    	\item Manifold Learning
	\end{itemize}
	\item Improve Generalization Capability from outside (not from inside the model)
	\begin{itemize}
    	\item Regularization Methods(eg. Ridge, Lasso)
    	\item Ensemble Learning(Stacking, Aggregating): Random Forest, Boosting(GBDT), Deep Learning...
	\end{itemize}
 \end{itemize}


\section{Linear Regressions}

Common Terms

\begin{enumerate}
    \item Independent Variable=Features=covariates
    \item Dependent Variable=
\end{enumerate}


\subsection{Assumptions}
Classic Assumptions for Statistics:
\begin{enumerate}
    \item Linear Relationship between covariates and dependent variable
    \item $E(\varepsilon)=0$
    \item $Var(\varepsilon) =\sigma^2$: Homoscedasticity
    \item $\varepsilon$ is independent with covariates 
    \item x is observed without error
    \item (optional, Gauss-Markov Theorm) $\varepsilon$ is normal - when it is, OLS and MLE agrees and to be BLUE(Best Linear Unbiased Estimator)
\end{enumerate}

\subsection{Intepretaion}


Under Normal Condition, we have
$$y \sim N(\beta_0 + \beta x_i, \sigma^2), L(\theta) = (\frac{1}{\sqrt{2\pi} \sigma)}^n exp( - \frac{\sum_{i=1}^n (y_i - (\beta_0 +\beta_1x_i))^2}{2\sigma^2})$$ 
Equivalent to minimize
$$RSS(\theta) = \sum_{i=1}^n (y_i - (\beta_0 +\beta_1x_i))^2$$
$$ \partial_{\beta_i} RSS = 0, i =1,2$$, we get
$$ r_{xy} = \frac{s_{xy}}{s_xs_y}, \beta_1 = r_{xy}\frac{s_y}{s_x} =\frac{s_{xy}}{s_x^2},\beta_0 = \bar{y}-\hat{\beta}\bar{x} $$

In Multi-variate Case:

$$ f(x) = \mathbf{w}^T \mathbf{x} = \sum_{i=1}^n w_i x_i $$
$$ \mathbf{w*} = (\mathbf{X^T X})^{-1} \mathbf{X^T y} $$

RSS Approach:


MLE Approach

Assuming noise is normal, maximize 

$$p(\mathbf{x_1, x_2 ... x_n}| \mathbf{ w} ) = \prod_k \frac{1}{\sqrt{2\pi} \sigma} exp[ -\frac{1}{2\sigma^2} (y_k - \mathbf{w_t x_k} )^2 ]$$ 

\subsection{Lasso-Least Absolute Shrinkage and Selection Operator}

$$ min ||y_k - \mathbf{w^T x}_k ||^2 + \lambda ||\mathbf{w}||_1 $$

\section{Regularization, Ridge and Lasso}

\chapter{Logistic Regression and Generalized Linear Model}

\chapter{Neutral-Network}

\chapter{Distance and Kth Nearest Neighbors}

\chapter{Naive Bayesian}

\chapter{Tree Models and Ensemble Learning}

Stacking Aggregating: Random Forest, Boosting(GBDT), Deep Learning...

\part{Unsupervised Learning Models}

\chapter{Clustering}

\chapter{Dimension Reduction}
\section{PCA}
\section{LDA}


\part{Deep Learning and Enhanced Learning Theory}

\chapter{Multi-layer Perceptron}



 
\end{document}                          % The required last line 