\documentclass[11pt, openany]{book}              % Book class in 11 points
\parindent0pt  \parskip10pt             % make block paragraphs
\raggedright                            % do not right justify
\usepackage{amsmath}

\title{\bf Machine Learning Handbook}    % Supply information
\author{Xinhe Liu}              %   for the title page.
\date{2018-2-28}                           %   Use current date. 

% Note that book class by default is formatted to be printed back-to-back.
\begin{document}                        % End of preamble, start of text.
% \frontmatter                            % only in book class (roman page #s)
\maketitle                              % Print title page.
\tableofcontents                        % Print table of contents
\mainmatter                             % only in book class (arabic page #s)

\part{High-level Views}
\chapter{Math Review}

\section{Linear Algebra}

Concepts:

\begin{itemize}
    \item scalar, vector, matrix, tensor(n-rank tensor, matrix is a rank 2 tensor) 
    \item Gaussian Elimination, rank
    \item p-norm $$|X|_p = (\sum_i |x_i|^p)^{\frac{1}{p}}$$
    \item inner product$<x_i,y_i>$, outer product
    \item orthogonal，dimension, basis, orthogonal basis
    \item linear transformation $Ax = y$
    \item eigenvalue, eigenvector $Ax =\lambda x$ (transformation and speed)
    \item vector space, linear space(with summation, scalar production), inner product space( inner product space) 
\end{itemize}


\section{Probability}

Concepts:

\begin{itemize}
    \item Classic Probability Model: Frequentist 
    \item Bayesian Probability Theory 
    \item Random variable, continuous RV, discrete RV, probability mass function, probability density function, cumulative density function
    \item Bernoulli distribution, Binomial distribution(n,p)$$P(X=k) = {N\choose k} p^k (1-p)^{(n-k)}$$, Poisson distribution $$P(X=k) = \lambda^k \frac{e^{-\lambda}}{k!}$$
    \item uniform distribution, exponential distribution $$e^{-\frac{x}{\theta}}\theta, P(x>s+t|X>s) = P(x>t)$$, normal distribution, t-distribution
    \item expectation, moments, variance, covariance, correlation coefficient
\end{itemize}


Theorems:

\begin{itemize}
    \item Law of Total Probability
    \item Bayesian Theorem $$P(H|D) = \frac{P(D|H) P(H)}{P(D)}$$ P(H)-prior probability, P(D|H)-likelihood, P(H|D)-posterior probability,
\end{itemize}

\subsection{Important Distributions, Moment Generating Functions}

\begin{enumerate}
	\item Normal Distribution, See next chapter
	\item Bernoulli Distribution 
	\item Exponential Distribution $f_x(x,\theta) = \theta e^{-\theta x} $
	\item Poisson Distribution
	
\end{enumerate} 
\section{Information Theory}

Concepts:

\begin{itemize}
    \item Information $$h(A) = -log_2 p(A)$$ (bit)
    \item (Information Source) Entropy $$ H(X) = -\sum_{i=1}^n p(a_i)log_2p(a_i) \leq log_2 n$$ Maximize under equal probability
    \item Conditional Entropy $$H(Y|X)  = -\sum_{i=1}^n p(x_i)H(Y|X=x_i)= -\sum_{i=1}^n p(x_i)\sum_{j=1}^n p(y_j|x_i) log_2p(y_j|x_i) $$ $$ = \sum_{i=1}^n \sum_{j=1}^n p(x_i,y_j) log_2p(y_j|x_i) $$
    \item Mutual Information/Information Gain $$I(X;Y) = H(Y) - H(Y|X)$$
    \item Kullback-Leibler Divergence (K-L) Divergence 
      $$D_{KL}(P||Q) = \sum_{i=1}^n p(x_i) log_2\frac{p(x_i)}{q(x_i)} \neq D_{KL}(Q||P)$$
      $$D_{KL}(f,\hat{f})) = \int_{-\infty}^{\infty} log(\frac{f_X(x)}{\hat{f(x)}})f_X(x)dx $$
    Measures the Distance of two distributions. The optimal encoding of information has the same bits as the entropy. Measures the extra bits if the real distribution is q rather than p. (Using P to approximate Q) K-L divergence plays an important role in both information theory and MLE theory. MLE $\hat{\theta}$ is actually finding the closest K-L Distance approximation of $f(x;\theta)$ to sample distribution.
\end{itemize}


Theorems:

\begin{itemize}
    \item The Maximum Entropy Principle. Without extra assumption, max entropy/equal probability has the minimum prediction risk. 
\end{itemize}
 
\section{Optimization Theory}

\begin{itemize}
    \item Objective function/Evaluation function, constrained/unconstrained optimization，Feasible Set, Optimal Solution, Optimal Value, Binding Constraints, Shadow Price, Infeasible Price, Infeasibility, Unboundedness    
    \item Linear Programming
    \item Lagrange Multiplier $$L(x,y,\lambda) = f(x,y) + \lambda \varphi(x,y) $$
    \item Convex Set, Convex Function $f:S\to R$ is convex if and only if $\bigtriangledown^2 f(\mathbf{x})$ is positive semidefinite 
\end{itemize}

Optimization Methods:
\begin{itemize}
    \item Linear Search Method: Direction First, Step Size second	
    	\begin{itemize}  		
    		\item Gradient Descent: Batch Processing(Use all samples) vs Stochastic Gradient Descent(Use one sample)
    		\item Newton's Method: Use Curvature Information 
   		\end{itemize}
   	\item Trust Region: Step first, direction second. Find optimal direction of second-order approximation. If the descent size is too small, make step size smaller.
   	\item Heuristics Method
   		\begin{itemize}  		
	    	\item Genetic Algorithm
	 	   	\item Simulated Annealing 
	    	\item Partical Swarming/Ant Colony Algorithm
	    \end{itemize}
\end{itemize}

Theorems:

\begin{itemize}
	\item
\end{itemize}


\section{Formal Logic}

Concepts
\begin{itemize}
    \item Generative Expert System: Rule+Facts+Deduction Engine 
    \item Godel's incompleteness theorems
\end{itemize}

\chapter{Statistics}

\section{Concepts}
\subsection{Basic}
\begin{itemize}
    \item parameter(constant for probability model), statistic (model of sample data), data, sample, population
    \item point estimation, interval estimation, Confidence Interval( $P(L \leq \theta \leq U )$, notice: $\theta$ is not random, L, U is random! ( We repeat constructing confidence interval a n times, $\alpha$ percent of the times, it will contain $theta$.
\end{itemize}

\subsection{Estimator and Estimation}

\begin{itemize}
    \item Method of Moments: $E(X^k)$ based on LOLN.\\ 
    	If We have p parameters, we can use p moments to form a system of equations to solve $\theta_1,...\theta_p$
    	$$\sum_{i=1}^n X_i^j= E(X^j)$$, for j = 1,...,p
    \item Maximum Likelihood Estimation. Multiply p.m.f/p.d.f since every sample is independent. Maximize the likelihood of finding samples. \\ If $X_1,...X_n \stackrel{i.i.d}{\sim} f_x(x, \theta)$, 
    	$$ l(\theta) =  \prod_{i=1}^n f_{X_i} f_{x_i} (x_i; \theta), L(\theta ) = log l(\theta)$$
    	$$ \hat{\theta}_{MLE}= argmax_{\theta} f_x(x;\theta ) = argmax_{\theta} L(\theta )$$
    	Analytical or Numerically solved. $$ \frac{\partial}{\partial \theta} [log L(\theta ) ] = 0, \frac{\partial^2}{\partial \theta^2} [log L(\theta ) ] < 0$$, for multiple parameters, we need the Hessian matrix to be negative definite $x^tHx<0, \forall x$ \\
    \item Properties of MLE
    	\begin{enumerate}
    		\item Invariance $\hat{\theta}$ is MLE of $\theta$, then $g(\hat{\theta})$ is MLE of $g(\theta)$
    		\item Consistency $$P(\hat{\theta} - \theta ) \rightarrow 0$$ as $n \rightarrow 0, \forall \epsilon > 0$ Under the conditions
    		\begin{enumerate}
    			\item $X_1,...X_n \stackrel{i.i.d}{\sim} f_x(x|\theta)$
    			\item parameters are identifiable, $\theta \neq \theta', f_x(x|\theta) \neq f_x(x|\theta')$
    			\item densities $f_x(x|\theta)$ has common support(set of x with positive density/probability), $f_x(x|\theta)$ is differentiable at $\theta$
    			\item parameter space $\Omega$ contains open set $\omega$ where true $\theta_0$ is an interior point
    		\end{enumerate}
    		\item Asymptotic Normality 
    		$$ \sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \rightarrow N(0, I^{-1}(\theta_0)) $$
    		$$ I(\theta_0) = E( -(\frac{\partial}{\partial \theta} [log f(x, \theta ) ])^2)=E(-\frac{\partial^2}{\partial \theta^2} [log f(x, \theta ) ] )$$ called the Fisher Information
    		$$ \hat{\theta}_{MLE} \approx N(\theta_0, \frac{1}{n I(\theta_0)})$$
    		$$ n I(\theta_0) = E( -\frac{\partial^2}{\partial \theta^2} log L(\theta) )$$
    		So the Variance of MLE( $1/ E( -\frac{\partial^2}{\partial \theta^2} log L(\theta) )$) is the reciprocal of amount of curvature at MLE. \\
    		Usually, We can just use the \textit{observed Fisher Information} (curvature near $\theta_{MLE}$) instead. ($I(\theta_{MLE})$) \\
    		$\frac{1}{ n I(\theta_0)}$ is called Cramer-Rao Lower Bound. \\ Under Multi-dimensional Case, 
    		
    		$$ I(\theta_0)_{ij} = =E(-\frac{\partial^2}{\partial \theta_i \partial \theta_j} [log f(x, \theta ) ] )$$ $Hessian \approx nI(\theta_0)$ $Hessian^{-1} \approx nI(\theta_0)$ when we use numerical approach. 

    		Under the above four conditions plus 
    		\begin{enumerate}
    			\item $ \forall x \in \chi$, $f_x(x|\theta)$ is three times differentiable with respect to $\theta$, and third derivative is continuous at $\theta$, and $\int f_x(x|\theta) dx$ can be differentiated three times under integral sign
    			\item $ \forall \theta \in \Omega, \exists c, M(x)$ (both depends on $\theta_0$) such that
    			$$ \frac{\partial^3}{\partial \theta^3} [log f(x, \theta ) ] \leq M(x), \forall x \in \chi, \theta_0-c<\theta < \theta_0+c, E_{\theta_0} [M(x)] < \infty$$
    		\end{enumerate}
    	\end{enumerate}
    \item $\Delta$ -Method: $g(\hat{theta}_{MLE})$ is approximately
    	  $$ N(g(\theta), (g'(\theta))^2 \frac{1}{nI(\theta)})$$ if asymptotic normality is satisfied. \\
    	  In Multivariate Case:
    	  $$ \hat{\theta} \sim N(\theta, \Sigma/n), \theta,\hat{\theta} \in R^p $$
    	  $$g: R^p \rightarrow R^m$$
    	  $$g(\hat{\theta}) \sim N(g(\theta), G\Sigma G^T/n)$$
		  $$ G = \begin{pmatrix} 
  				\frac{\partial{g_1(\theta)}}{\partial{\theta_1}}& \cdots & \frac{\partial{g_1(\theta)}}{\partial{\theta_p}}\\ 
  				\vdots & \ddots & \vdots \\
  				\frac{\partial{g_m(\theta)}}{\partial{\theta_1}}& \cdots & \frac{\partial{g_m(\theta)}}{\partial{\theta_p}} 
			\end{pmatrix} $$
    \item Estimation criteria
   		\begin{itemize}
   		 \item Unbiased $E(\hat{\theta}) = \theta$
   		 \item Minimum Variance (MVUE, minimum variance unbiased estimator) $Var(\hat{\theta}) < Var(\theta')$
   		 \item Efficient
   		 \item Coherent
   		 \end{itemize}
\end{itemize}

\subsection{Model Selection}

AIC - Akaike Information Criterion

By K-L Distance
$$D_{KL}(f,\hat{f})) = \int_{-\infty}^{\infty} log(\frac{f_X(x)}{\hat{f(x)}})f_X(x)dx $$
$$ = const + \frac{1}{2} \int (-2log\hat{f}(x))f(x)dx = const + AIC $$
$$ A(f,\hat{f}) = -2 logL(\theta) + 2 p (\frac{n}{n-p+1})$$

\subsection{Hypothesis Testing}

\begin{itemize}
    \item Hypothese, Test Statistic, Rejection Region
    \item p-value ( chance of rejecting, largest choice of $\alpha$ that we would fail to reject $H_0$)   
    \item type-I error(wrongly reject), type-II error(wrongly accept)
\end{itemize}

\section{Theorems}

\begin{itemize}
    \item Law of Large Number
    \item Central Limit Theorem
        \item Bias/Variance decomposition (error = bias + variance + noise)
        $$MSE(\mu(X) ) = E[(Y-\hat{\mu}(X))^2]  = = E[(Y-f(x) + f(x) -\hat{\mu}(X))^2]$$
    $$= E[(Y-f(x)]^2 + 2E[(Y-f(x))(f(x) - \hat{\mu}(X))] + E[(f(x)-\hat{\mu}(X))^2]$$
     $$= E[(Y-f(x)]^2 + 2E[(Y-f(x))(f(x) - \hat{\mu}(X))] + (f(x)-\hat{\mu}(X))^2$$
    $$ =\sigma_x^2 + Bias(\hat{\mu}(X))^2 + Var(\hat{\mu}(X))$$
 
\end{itemize}

\section{Important Distributions}

\begin{enumerate}
	\item Normal Distribution, $X_1,...X_n \sim N(\mu,\sigma^2)$ then
	\begin{enumerate}
		\item  $\bar{X}$ and $s^2$ are independent
		\item  $\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)$
		\item  $\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2$
		\item  $\frac{\bar{X}-\mu }{s/\sqrt{n}} = \frac{\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}}{\frac{(n-1)s^2}{\sigma^2} \frac{1}{\sqrt{n-1}}} \sim t_{n-1}$
	\end{enumerate}
	\item Multi-variate normal distribution
	$$ f_x(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$
	\begin{enumerate}
		\item $X_1,...X_n$ normal $\Leftarrow (X_1,...X_n)$ is multivariate normal. (Not equivalent)
		\item $E(X)=\mu, Var(X) = \Sigma$
		\item Linear transformations $AX+b \sim N(A\mu+b, A\Sigma A^T)$ remain multivariate normal
		\item Marginals are multivariate normal, each sub-vector is multivariate normal, the parameters are just sub-matrices.
		\item All conditionals are multivariate normal 
	\end{enumerate}
	\item t-distribution: like normal distribution, but heavier tails
	\begin{enumerate}
		\item $Z \sim N(0,1), Y \sim \chi^2_{\nu}$, Z, Y independent, $$ X= Z/\sqrt{Y/\nu} \sim t_{\nu} $$
		\item pdf has polynomial tails (decays much slower than exponential ones)
		\item $\nu=1$, it is the \textbf{Cauchy Distribution}, with very heavy tails (no expectation)
		\item The MCF not exist. $E(|X|^k) < \infty$ for $k<\nu$, $E(|X|^k) = \infty$ for $k>\nu$
		\item $X\sim t_\nu, E(X)=0, Var(X)=\frac{\nu}{\nu-2}$
		$$ f_X(x)=\frac{1}{\pi(1+x^2)}$$
	\end{enumerate}
	\item $\chi^2$ distribution
	$$ f_x(x) = \frac{1}{(2^{k/2}\Gamma(k/2)}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}, x\in [0,\infty) \sim Gamma(\frac{k}{2},\frac{1}{2})$$
	\begin{enumerate}
		\item $E(X)=k, Var(X)=2k, M_X(t)= (\frac{1}{1-2^t})^{k/2}$
		\item $X \sim N(0,1) \Rightarrow X^2 \sim \chi^2$, $X_1,...X_n \sim N(0,1) i.i.d \Rightarrow \sum X_i^2 \sim \chi^2$,
		$$ f_X(x)=\frac{1}{\pi(1+x^2)}$$
	\end{enumerate}	
\end{enumerate}

More Generalized Distributions

\begin{enumerate}
	\item Generalized Error Distribution (symmetric)
	\item Non-standard t-distribution (shift and scaling, heavy tailed, symmetric)
	\item Theodossious skewed t-distribution
	\item Theodossious skewed t-distribution plus shift
\end{enumerate}



   
\section{Practice/Examples}

\begin{enumerate}

    \item sample mean($\bar{X}$) is unbiased. Sample variance ($\frac{1}{n-1}\sum_{i=1}^n x_i^n$) is unbiased. But sample std is not unbiased. $SE(\bar{X})=\frac{\sigma^2}{n}$
    \item $\hat{Cov}(X.Y) = \frac{1}{n-1} \sum_{i=1}^n(X_i-\bar{X})(X_i-\bar{Y})$ is unbiased
    \item Distributions with Expectation not exist? (Cauchy) 
    \item Common Confidence Intervals:\\ $\mu$: $P(-t_{\alpha/2, n-1} \leq \frac{\bar{X}-\mu}{s/\sqrt{n}} \leq t_{\alpha/2, n-1})= 1- \alpha$, \\$\sigma$: $P(a \leq \frac{(n-1)s^2}{\sigma^2} \leq b)= 1-\alpha$
    \item Solve MLE/MOM for beta, exponential ($n/\sum{X_i}$, normal 
    \item * prove Asymptotic Normality of MLE( hint: using Taylor Expansion for $\theta, \hat{\theta}$ )
    \item * Use $t^{th}$ quantile to approximate c.d.f, what's the distribution? ($Y_n=\frac{1}{n} \sum I(X_i <x)$, a Bernoulli distribution with $p=F_x(x)$, $\sqrt{n}[Y_n(x) - F_x(x)] \sim N(0, F(x)(1-F(x))$.
 	\item $X_1, .... X_n \sim Binomial(n,p)$, What's the MLE for p and Fisher Information? ( $\hat{p}= \frac{x_i}{n}, I(p) = 1/p(1-p), var(p) = \frac{p(1-p)}{n}$)
 	\item $(x_i,y_i) \sim N(\mu_i, \sigma^2)$, find MLE for $\sigma$ ( $\frac{1}{4N} \sum(x_i-y_i)$ )
 	\item How can you get N(0,1) random variables from U[0,1]? ( Method1: Inverse Transformation, Method2; Use $Sum Z_i^2 \sim \chi_k^2, k = 2, F^{-1}(u) = -2log(1-u)$, $R^2 \sim \chi^2, Z_1 = Rcos\theta, Z_2 = Rsin\theta, \theta \in [0, 2\pi]$
\end{enumerate}


\chapter{Computational Learning Theory}

\part{Supervised Learning Models}

\chapter{Regression}

\section{Linear Regressions}


\subsection{Assumptions}
Classic Assumptions for Statistics:
\begin{itemize}
    \item 
\end{itemize}

\subsection{Inteprataion}

$$ f(x) = \mathbf{w}^T \mathbf{x} = \sum_{i=1}^n w_i x_i $$
$$ \mathbf{w*} = (\mathbf{X^T X})^{-1} \mathbf{X^T y} $$

RSS Approach:


MLE Approach

Assuming noise is normal, maximize 

$$p(\mathbf{x_1, x_2 ... x_n}| \mathbf{ w} ) = \prod_k \frac{1}{\sqrt{2\pi} \sigma} exp[ -\frac{1}{2\sigma^2} (y_k - \mathbf{w_t x_k} )^2 ]$$ 

\subsection{Lasso-Least Absolute Shrinkage and Selection Operator}

$$ min ||y_k - \mathbf{w^T x}_k ||^2 + \lambda ||\mathbf{w}||_1 $$

\chapter{Logistic Regression and General Linear Model}

\chapter{Naive Bayesian}

\chapter{Tree Models and Ensemble Learning}

\part{Unsupervised Learning Models}

\chapter{Clustering}

\chapter{Dimension Reduction}



\part{Deep Learning and Enhanced Learning Theory}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

\chapter{Multi-layer Perceptron}

 
\end{document}                          % The required last line 