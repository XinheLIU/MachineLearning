---
layout: page
title: "Math Fundamentals"
author: Xinhe LIU
date: 2020-08-13 15:00:00 -0000
categories: Math
# tags: []
# image
---

- [High School Math](#high-school-math)
  - [Gemetry](#gemetry)
  - [Algebra](#algebra)
    - [Important Inequalities](#important-inequalities)
- [Calculus](#calculus)
- [Probability](#probability)
  - [Classic Probability Model](#classic-probability-model)
  - [Measure based probability](#measure-based-probability)
- [Optimization Theory](#optimization-theory)
  - [Optimization Methods](#optimization-methods)
- [Information Theory](#information-theory)
- [Formal Logic](#formal-logic)

## High School Math

### Gemetry

- [Law of Sines](https://en.wikipedia.org/wiki/Law_of_sines)
- [Law of Cosines](https://en.wikipedia.org/wiki/Law_of_cosines)

### Algebra

#### Important Inequalities

- Distance Inequality 
  - all "distance measures" should satisfy 
  - $$ |\mathbf{x} + \mathbf{y}| \leq |\mathbf{x}| + |\mathbf{y}|$$
- Cauchy-Shwartz 
  - $$|\mathbf{xy}| \leq |\mathbf{x}||\mathbf{y}|$$
- Jensen's Inequality 
  - $$f((E(x)) \leq E(f(x))$$
  - for convex f
- Infinite Series 
  - $$ e^{-x^p} \leq \frac{c}{x^q}, \exists c,p,q, \forall x \geq a$$ 

## Calculus

Concepts to know

Differentiation

- Derivatives:
  - product, quotient, chain rule
  - $x^n, sinx, cosx, tanx, a^x, lnx$ 
- Limits 
  - $$e^x = \lim_{n\to \infty}(1+\frac{x}{n})^n$$
  - $$\lim_{x\to 0} \frac{sinx}{x} = 1$$
- [L'Hospital's Rule](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule)
- Second Derivative Test 
  - in multivariate case, Hessian Matrix is positive definitive
- Mean-Value Theorem
	- Rolle, Cauchy
- [Taylor's Theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem)
  - Maclaulin Seris: 
    - $sinx, cosx, e^x, \frac{1}{1-x}$
    - $(1+x)^k, ln(1+x)$

Integral

- Integration
  - Remann Integral
  - Integral Calculuation
    - Substitution Rule - Change of variable
    - Integration By Parts
- [Fundamental Theorem of Calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus)
	- [Lebnitz Rule for Integration](https://en.wikipedia.org/wiki/Leibniz_integral_rule)
- Series, Indefinite Integral
	- Convergence and Divergence
	- Comparison Test, Limit Comparison Test, Ratio Test

Multi-variate Calculus

- Multi-variate calculus
  - Jacobian, Hessian Matrix
  - Fubini's Rule
  - Polar Coordinate ($\Delta A = rdrd\theta$), Spherical Coordinate
- Ordinary Derivative Equations
  - Separation of Variable
  - First Order O.D.E, integrating factor
  - Second Order O.D.E, characteristic root
- Partial Derivative Equations

## Probability

### Classic Probability Model

- Random variable
  - continuous RV
  - discrete RV
  - probability mass function
  - probability density function, cumulative density function
- Event, Count, Probability
- Permutation, Combination
- [Binomial Theorem](https://en.wikipedia.org/wiki/Binomial_theorem)
- [Inclusion-Exclusion Principles](https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle)
- [Expectation](https://en.wikipedia.org/wiki/Expected_value)
- Independence
- moments, variance, covariance
- Correlation
  - [Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)
  - [Rank Correlation](https://en.wikipedia.org/wiki/Rank_correlation)
    - [Spearman's correlation](https://en.wikipedia.org/wiki/Rank_correlation)
  - Consine similarity (correlation in Euclidean space)
    $$cos\theta = \frac{\mathbf{A}^T\mathbf{B}}{\|\mathbf{A}\|}\|{\mathbf{B}}\|	$$
- [Moment Generating Functions](https://en.wikipedia.org/wiki/Moment-generating_function)
  - $M_X(t) = E(e^{tX})$

### Measure based probability

Probability Space $(\Omega, \mathcal{F},P)$ 

- Sample Space(collection of outcomes)
- [Sigma-algebra](https://en.wikipedia.org/wiki/%CE%A3-algebra?wprov=sfti1)(events)
  - Borel sets
  - Filtration
    - Adapted Filtration
- Probability measure (assigns probability to events)
  - [Measuarable](https://en.wikipedia.org/wiki/Measurable_space?wprov=sfti1)

Key concepts expaned to measures

- independence
  - [Randon-Nickodym Theorem](https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem?wprov=sfti1)
- conditional probability
- conditional expectation
  - partial averaging properties
  - independence lemma

Theorems

- Law of Total Probability
- Bayes' Rule 
  - $$P(H|D) = \frac{P(D|H) P(H)}{P(D)}$$ 
  - $P(H)$-prior probability, $P(D|H)$-likelihood, $P(H|D)$-posterior probability

Important Distributions

- Bernoulli distribution
- Uniform Distributiondiscrete)
- Binomial distribution(n,p)$$P(X=k) = {N\choose k} p^k (1-p)^{(n-k)}$$ $$E(X) =np, Var(X) =np(1-p)$$
- Geometric Distribution $$P(x) = (1-p)^{x-1}p$$
  - $$E(X) = \frac{1}{p}, Var(X) =\frac{1-]}{p^2}$$
- Poisson distribution 
  - $$P(X=k) = \lambda^k \frac{e^{-\lambda}}{k!}$$ 
  - $$E(X) = \lambda, Var(X) = \lambda$$
- Negative Binomial Distribution
- Normal Distribution, See next chapter
- Bernoulli Distribution 
- Uniform Distribution(continuous)
- Exponential distribution 
  - $$e^{-\frac{x}{\theta}}\theta$$ 
  - $$P(x>s+t|X>s) = P(x>t)$$
  - $$E(x) = \frac{1}{\lambda}, Var(X) = \frac{1}{\lambda^2}$$
- [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)
  - [Generalized Normal Distribution](https://en.wikipedia.org/wiki/Generalized_normal_distribution)
- t-distribution
- Beta Distribution (the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions.)
- Gamma Distribution 
  - (The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution. ) 
  - (the gamma distribution is the conjugate prior to many likelihood distributions: the Poisson, exponential, normal (with known mean), Pareto, gamma with known shape σ, inverse gamma with known shape parameter)

## Optimization Theory

- Objective function/Evaluation function, constrained/unconstrained optimization，Feasible Set, Optimal Solution, Optimal Value, Binding Constraints, Shadow Price, Infeasible Price, Infeasibility, Unboundedness
- Linear Programming
- Lagrange Multiplier 
  - $$L(x,y,\lambda) = f(x,y) + \lambda \varphi(x,y) $$
- Convex Set, Convex Function 
  - $f:S\to R$ is convex if and only if $\bigtriangledown^2 f(\mathbf{x})$ is positive semidefinite 
- Duality - the equivalent problem of the primal problem.
- Quadratic programming (QP)
  -[Sequential Minimal Optimization(SMO)](https://en.wikipedia.org/wiki/Sequential_minimal_optimization) 

### Optimization Methods

- Linear Search Method: Direction First, Step Size second	
  - Gradient Descent: Batch Processing(Use all samples) vs Stochastic Gradient Descent(Use one sample)
    - $$\theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$$
  - Newton's Method: Use Curvature Information 
  $$\mathbf{\beta}^{t+1} = \mathbf{\beta}^t - (\frac{\partial^2 Loss(\mathbf{\beta})}{\partial \mathbf{\beta} \partial\mathbf{\beta}^T})^{-1} \frac{ \partial Loss(\mathbf{\beta})}{\partial \mathbf{\beta}} $$
- Trust Region: 
  - Step first, direction second. Find optimal direction of second-order approximation. If the descent size is too small, make step size smaller.
- Heuristics Methods
  - [Genetic Algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm)
  - [ Simulated Annealing ](https://en.wikipedia.org/wiki/Simulated_annealing)
  - [Ant Colony Algorithm](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms)

## Information Theory

- Information 
  - $$h(A) = -log_2 p(A)$$
    - (in bits)
- (Information Source) [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))
  - $$ H(X) = -\sum_{i=1}^n p(x_i)log_2p(x_i) = \sum_{i=1}^n p(x_i) \frac{1}{log_2p(x_i)}\leq log_2 n$$ 
    - Maximized under equal probability
- Conditional Entropy
  - $$H(Y|X)  = -\sum_{i=1}^n p(x_i)H(Y|X=x_i)= -\sum_{i=1}^n p(x_i)\sum_{j=1}^n p(y_j|x_i) log_2p(y_j|x_i)$$ 
  - $$ = \sum_{i=1}^n \sum_{j=1}^n p(x_i,y_j) log_2p(y_j|x_i) $$
- [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)/Information Gain 
  - $$I(X;Y) = D_{KL}(p(X,Y)||p(X)p(Y) = \sum_x \sum_yp(x,y) \frac{p(x,y)}{p(x)p(y)}$$
  - $$=H(Y) - H(Y|X) = H(X) - H(X|Y)$$
  -  $$=H(X) + H(Y) - H(X;Y) = H(X;Y) - H(X|Y) - H(Y|X)$$
- [Kullback-Leibler Divergence (K-L) Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
  - $$D_{KL}(P||Q) = \sum_{i=1}^n p(x_i) log_2\frac{p(x_i)}{q(x_i)} \neq D_{KL}(Q||P)$$
  - $$D_{KL}(f,\hat{f})) = \int_{-\infty}^{\infty} log(\frac{f_X(x)}{\hat{f(x)}})f_X(x)dx $$
  - K-L Divergence Measures the Distance of two distributions. The optimal encoding of information has the same bits as the entropy. Measures the extra bits if the real distribution is q rather than p. (Using P to approximate Q) 
  - K-L divergence plays an important role in both information theory and MLE theory. MLE $\hat{\theta}$ is actually finding the closest K-L Distance approximation of $f(x;\theta)$ to sample distribution.

Theorems

- The Maximum Entropy Principle. 
  - Without extra assumption, max entropy/equal probability has the minimum prediction risk. 

## Formal Logic

- **Generative Expert System**
  - Rule+Facts+Deduction Engine
- Godel's incompleteness theorems