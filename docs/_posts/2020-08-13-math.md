---
layout: page
title: "Math Fundamentals"
author: Xinhe LIU
date: 2020-08-13 15:00:00 -0000
categories: Math
# tags: []
# image
---

- [High School Math](#high-school-math)
  - [Gemetry](#gemetry)
  - [Algebra](#algebra)
    - [Important Inequalities](#important-inequalities)
- [Calculus](#calculus)
- [Probability](#probability)
- [Optimization Theory](#optimization-theory)
  - [Optimization Methods](#optimization-methods)
- [Information Theory](#information-theory)
- [Formal Logic](#formal-logic)

## High School Math

### Gemetry

- [Law of Sines](https://en.wikipedia.org/wiki/Law_of_sines)
- [Law of Cosines](https://en.wikipedia.org/wiki/Law_of_cosines)

### Algebra

#### Important Inequalities

- Distance Inequality 
  - all "distance measures" should satisfy 
  - $$ |\mathbf{x} + \mathbf{y}| \leq |\mathbf{x}| + |\mathbf{y}|$$
- Cauchy-Shwartz 
  - $$|\mathbf{xy}| \leq |\mathbf{x}||\mathbf{y}|$$
- Jensen's Inequality 
  - $$f((E(x)) \leq E(f(x))$$
  - for convex f
- Infinite Series 
  - $$ e^{-x^p} \leq \frac{c}{x^q}, \exists c,p,q, \forall x \geq a$$ 

## Calculus

- Derivatives: 
  - product, quotient, chain rule, $x^n, sinx, cosx, tanx, a^x, lnx$ 
- Limits 
  - $$e^x = \lim_{n\to \infty}(1+\frac{x}{n})^n, \lim_{x\to 0} \frac{sinx}{x} = 1$$
- L'Hospital's Rule
- Second derivative test 
  - in multivariate case, Hessian Matrix is positive definitive
- Mean-Value Theorem
	- Rolle, Cauchy 
- Taylor's Theorem
	- Maclaulin Seris: $sinx, cosx, e^x, \frac{1}{1-x}$
- Integration
  - Remann Integral
  - Substitution Rule, Integration By Parts
- Fundamental Theorem of Calculus
	- Lebnitz Rule for Integration
- Series, Indefinite Integral
	- Convergence and Divergence
	- Comparison Test, Limit Comparison Test, Ratio Test
- Multi-variate calculus
	- Jacobian, Hessian Matrix
	- Fubini's Rule
	- Polar Coordinate ($\Delta A = rdrd\theta$), Spherical Coordinate
- Ordinary Derivative Equations
	- Separation of Variable
	- First Order O.D.E, integrating factor
	- Second Order O.D.E, characteristic root
- Partial Derivative Equations

## Probability

- Classic Probability Model: Frequentist 
- Bayesian Probability Theory 
- Random variable, continuous RV, discrete RV, probability mass function, probability density function, cumulative density function
- Independence, Correlation
- Permutation, Combination, Binomial Theorem, Inclusion-Exclusion Principles
- expectation, moments, variance, covariance, correlation coefficient
  - Consine similarity (correlation in Euclidean space)
    $$cos\theta = \frac{\mathbf{A}^T\mathbf{B}}{\|\mathbf{A}\|}\|{\mathbf{B}}\|	$$
- Moment Generating Functions 

Measure based probability

$(\Omega, \mathcal{F},P)$ Sample Space(collection of outcomes), Sigma-algebra(events), Probability measure (assigns probability to events)

Theorems

- Law of Total Probability
- Bayes' Rule 
  - $$P(H|D) = \frac{P(D|H) P(H)}{P(D)}$$ 
  - $P(H)$-prior probability, $P(D|H)$-likelihood, $P(H|D)$-posterior probability

Important Distributions

- Bernoulli distribution
- Uniform Distributiondiscrete)
- Binomial distribution(n,p)$$P(X=k) = {N\choose k} p^k (1-p)^{(n-k)}$$ $$E(X) =np, Var(X) =np(1-p)$$
- Geometric Distribution $$P(x) = (1-p)^{x-1}p$$
  - $$E(X) = \frac{1}{p}, Var(X) =\frac{1-]}{p^2}$$
- Poisson distribution 
  - $$P(X=k) = \lambda^k \frac{e^{-\lambda}}{k!}$$ 
  - $$E(X) = \lambda, Var(X) = \lambda$$
- Negative Binomial Distribution
- Normal Distribution, See next chapter
- Bernoulli Distribution 
- Uniform Distribution(continuous)
- Exponential distribution 
  - $$e^{-\frac{x}{\theta}}\theta$$ 
  - $$P(x>s+t|X>s) = P(x>t)$$
  - $$E(x) = \frac{1}{\lambda}, Var(X) = \frac{1}{\lambda^2}$$
- [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)
  - [Generalized Normal Distribution](https://en.wikipedia.org/wiki/Generalized_normal_distribution)
- t-distribution
- Beta Distribution (the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions.)
- Gamma Distribution 
  \subitem (The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution. ) 
  \subitem (the gamma distribution is the conjugate prior to many likelihood distributions: the Poisson, exponential, normal (with known mean), Pareto, gamma with known shape σ, inverse gamma with known shape parameter)

## Optimization Theory

- Objective function/Evaluation function, constrained/unconstrained optimization，Feasible Set, Optimal Solution, Optimal Value, Binding Constraints, Shadow Price, Infeasible Price, Infeasibility, Unboundedness
- Linear Programming
- Lagrange Multiplier 
  - $$L(x,y,\lambda) = f(x,y) + \lambda \varphi(x,y) $$
- Convex Set, Convex Function 
  - $f:S\to R$ is convex if and only if $\bigtriangledown^2 f(\mathbf{x})$ is positive semidefinite 
- Duality - the equivalent problem of the primal problem.
- Quadratic programming (QP)
  -[Sequential Minimal Optimization(SMO)](https://en.wikipedia.org/wiki/Sequential_minimal_optimization) 

### Optimization Methods

- Linear Search Method: Direction First, Step Size second	
  - Gradient Descent: Batch Processing(Use all samples) vs Stochastic Gradient Descent(Use one sample)
    - $$\theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$$
  - Newton's Method: Use Curvature Information 
  $$\mathbf{\beta}^{t+1} = \mathbf{\beta}^t - (\frac{\partial^2 Loss(\mathbf{\beta})}{\partial \mathbf{\beta} \partial\mathbf{\beta}^T})^{-1} \frac{ \partial Loss(\mathbf{\beta})}{\partial \mathbf{\beta}} $$
- Trust Region: 
  - Step first, direction second. Find optimal direction of second-order approximation. If the descent size is too small, make step size smaller.
- Heuristics Methods
  - [Genetic Algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm)
  - [ Simulated Annealing ](https://en.wikipedia.org/wiki/Simulated_annealing)
  - [Ant Colony Algorithm](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms)

## Information Theory

- Information 
  - $$h(A) = -log_2 p(A)$$
  - (in bits)
- (Information Source) [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))
  - $$ H(X) = -\sum_{i=1}^n p(a_i)log_2p(a_i) \leq log_2 n$$ 
    - Maximized under equal probability
- Conditional Entropy
  - $$H(Y|X)  = -\sum_{i=1}^n p(x_i)H(Y|X=x_i)= -\sum_{i=1}^n p(x_i)\sum_{j=1}^n p(y_j|x_i) log_2p(y_j|x_i)$$ 
  - $$ = \sum_{i=1}^n \sum_{j=1}^n p(x_i,y_j) log_2p(y_j|x_i) $$
- Mutual Information/Information Gain 
  - $$I(X;Y) = H(Y) - H(Y|X)$$
- [Kullback-Leibler Divergence (K-L) Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
  - $$D_{KL}(P||Q) = \sum_{i=1}^n p(x_i) log_2\frac{p(x_i)}{q(x_i)} \neq D_{KL}(Q||P)$$
  - $$D_{KL}(f,\hat{f})) = \int_{-\infty}^{\infty} log(\frac{f_X(x)}{\hat{f(x)}})f_X(x)dx $$
  - K-L Divergence Measures the Distance of two distributions. The optimal encoding of information has the same bits as the entropy. Measures the extra bits if the real distribution is q rather than p. (Using P to approximate Q) 
  - K-L divergence plays an important role in both information theory and MLE theory. MLE $\hat{\theta}$ is actually finding the closest K-L Distance approximation of $f(x;\theta)$ to sample distribution.

Theorems

- The Maximum Entropy Principle. 
  - Without extra assumption, max entropy/equal probability has the minimum prediction risk. 

## Formal Logic

- **Generative Expert System**
  - Rule+Facts+Deduction Engine
- Godel's incompleteness theorems